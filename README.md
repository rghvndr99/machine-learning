# Machine Learning Examples

A comprehensive collection of machine learning examples organized by learning type, covering supervised and unsupervised learning algorithms with practical implementations using Python, scikit-learn, pandas, and Jupyter notebooks.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Repository Structure](#repository-structure)
- [Supervised Learning](#supervised-learning)
- [Unsupervised Learning](#unsupervised-learning)
- [Advanced Topics](#advanced-topics)
- [Technologies Used](#technologies-used)
- [Getting Started](#getting-started)
- [Quick Start Examples](#quick-start-examples)

## ğŸ¯ Overview

This repository contains hands-on machine learning examples organized into two main categories:

### **Supervised Learning**
- **Linear Regression**: Single and multiple variable models
- **Logistic Regression**: Classification problems
- **Decision Trees & Random Forest**: Tree-based algorithms
- **Support Vector Machines**: SVM for classification and regression
- **K-Nearest Neighbors**: Instance-based learning
- **Model Evaluation**: Cross-validation, train/test splits

### **Unsupervised Learning**
- **K-Means Clustering**: Centroid-based clustering
- **Naive Bayes**: Probabilistic classification
- **Principal Component Analysis (PCA)**: Dimensionality reduction

### **Advanced Topics**
- **Regularization**: L1 & L2 regularization techniques
- **Hyperparameter Tuning**: Grid search and optimization
- **Ensemble Learning**: Combining multiple models
- **Outlier Detection**: IQR-based outlier removal

## ğŸ“ Repository Structure

```
machine-learning/
â”œâ”€â”€ superwised/                     # Supervised Learning Examples
â”‚   â”œâ”€â”€ linear-reg/                # Linear regression examples
â”‚   â”œâ”€â”€ linear-reg-multivalue/     # Multiple linear regression
â”‚   â”œâ”€â”€ dummy_variable_and one_hot_encoding/  # Categorical data preprocessing
â”‚   â”‚   â”œâ”€â”€ homeprices.csv         # Home prices with categorical data
â”‚   â”‚   â”œâ”€â”€ carprices.csv          # Car prices with categorical data
â”‚   â”‚   â”œâ”€â”€ pandas_sklearn_onehot_encoding_method.ipynb  # Modern approach
â”‚   â”‚   â”œâ”€â”€ prediction_example.py  # Complete prediction example
â”‚   â”‚   â””â”€â”€ README_PREDICTIONS.md  # Detailed prediction guide
â”‚   â”œâ”€â”€ logistic_regression/       # Classification examples
â”‚   â”œâ”€â”€ logistic_regression-multivalue/  # Multi-class classification
â”‚   â”œâ”€â”€ decesion_tree/            # Decision tree algorithms
â”‚   â”œâ”€â”€ random-forest/            # Random forest ensemble
â”‚   â”œâ”€â”€ support_vector_machine/   # SVM examples
â”‚   â”œâ”€â”€ k-fold/                   # Cross-validation examples
â”‚   â”œâ”€â”€ k-fold_for_iris-data-set/ # K-fold with Iris dataset
â”‚   â”œâ”€â”€ test_and_train_data_Set/  # Train/test splitting
â”‚   â”œâ”€â”€ save-linear-reg-model/    # Model persistence
â”‚   â”œâ”€â”€ load_digit_data_set/      # Digit recognition dataset
â”‚   â””â”€â”€ Employee_retention_modal/ # Employee retention prediction
â”œâ”€â”€ unsuperwised/                  # Unsupervised Learning Examples
â”‚   â”œâ”€â”€ k mean clustering/        # K-means clustering
â”‚   â”œâ”€â”€ naive-based-spam-detection/  # Spam detection with Naive Bayes
â”‚   â””â”€â”€ naive-based-titatic-usecase/  # Titanic survival prediction
â”œâ”€â”€ L1&L2Regulisation/            # Regularization techniques
â”œâ”€â”€ hyper-params-tunning/         # Hyperparameter optimization
â”œâ”€â”€ insamble-learning/            # Ensemble methods
â”œâ”€â”€ outlier-removal-IQR/          # Outlier detection and removal
â”œâ”€â”€ pca/                          # Principal Component Analysis
â”œâ”€â”€ knn/                          # K-Nearest Neighbors
â””â”€â”€ README.md
```

## ğŸ“ Supervised Learning

### ğŸ“ˆ Regression Examples

#### Linear Regression (`superwised/linear-reg/`)
- **Single Variable**: Home price prediction based on area
- **Time Series**: Canada per capita income analysis (1970-2016)
- **Dataset**: `homeprices.csv`, `canada_per_capita_income.csv`
- **Key Concepts**: Simple linear regression, trend analysis

#### Multiple Linear Regression (`superwised/linear-reg-multivalue/`)
- **Multi-Variable**: Home price prediction with area, bedrooms, age
- **Data Preprocessing**: Missing value handling, data imputation
- **Advanced Features**: Feature selection, correlation analysis

#### Categorical Data Preprocessing (`superwised/dummy_variable_and_one_hot_encoding/`)
- **Modern Approach**: Scikit-learn `OneHotEncoder` with `ColumnTransformer`
- **Complete Pipeline**: Data preprocessing, model training, predictions
- **Practical Examples**:
  - **Standalone Script**: `prediction_example.py` - Complete workflow demonstration
  - **Jupyter Notebook**: Step-by-step categorical data handling
  - **Prediction Guide**: `README_PREDICTIONS.md` - Comprehensive documentation
- **Key Features**:
  - Production-ready preprocessing pipelines
  - Multiple prediction scenarios (single, batch, comparison)
  - Reusable prediction functions
  - Model interpretation and coefficient analysis
  - Error handling and best practices

### ğŸ¯ Classification Examples

#### Logistic Regression (`superwised/logistic_regression/`)
- **Binary Classification**: Insurance purchase prediction
- **Dataset**: `insurance_data.csv`
- **Key Concepts**: Sigmoid function, probability prediction, decision boundaries

#### Multi-Class Logistic Regression (`superwised/logistic_regression-multivalue/`)
- **Multi-Class Classification**: Extended classification problems
- **Advanced Features**: One-vs-rest, multinomial classification

#### Decision Trees (`superwised/decesion_tree/`)
- **Tree-Based Learning**: Decision tree algorithms
- **Interpretability**: Visual decision paths, feature importance

#### Random Forest (`superwised/random-forest/`)
- **Ensemble Method**: Multiple decision trees
- **Improved Accuracy**: Reduced overfitting, better generalization

#### Support Vector Machine (`superwised/support_vector_machine/`)
- **SVM Classification**: Linear and non-linear classification
- **Kernel Methods**: RBF, polynomial kernels

#### K-Nearest Neighbors (`knn/`)
- **Instance-Based Learning**: Lazy learning algorithm
- **Distance Metrics**: Euclidean, Manhattan distance

### ğŸ”„ Model Evaluation & Validation

#### Cross-Validation (`superwised/k-fold/`)
- **K-Fold Validation**: Model performance evaluation
- **Bias-Variance Tradeoff**: Understanding model generalization

#### Iris Dataset Example (`superwised/k-fold_for_iris-data-set/`)
- **Classic Dataset**: Iris flower classification
- **Complete Pipeline**: Data loading, preprocessing, validation

#### Train/Test Splitting (`superwised/test_and_train_data_Set/`)
- **Data Splitting**: Proper train/test separation
- **Performance Metrics**: Accuracy, precision, recall, F1-score

### ğŸ’¾ Model Persistence (`superwised/save-linear-reg-model/`)
- **Joblib Method**: Efficient model serialization
- **Pickle Method**: Standard Python serialization
- **Model Deployment**: Loading and using saved models

### ğŸ¢ Real-World Applications

#### Employee Retention (`superwised/Employee_retention_modal/`)
- **HR Analytics**: Predicting employee turnover
- **Business Impact**: Data-driven HR decisions

#### Digit Recognition (`superwised/load_digit_data_set/`)
- **Computer Vision**: Handwritten digit classification
- **Feature Engineering**: Image data preprocessing

## ğŸ” Unsupervised Learning

### ğŸ¯ Clustering

#### K-Means Clustering (`unsuperwised/k mean clustering/`)
- **Centroid-Based Clustering**: Grouping similar data points
- **Applications**: Customer segmentation, data exploration
- **Key Concepts**: Elbow method, cluster validation

### ğŸ“Š Probabilistic Models

#### Naive Bayes - Spam Detection (`unsuperwised/naive-based-spam-detection/`)
- **Text Classification**: Email spam detection
- **Probabilistic Learning**: Bayes theorem application
- **Feature Engineering**: Text preprocessing, TF-IDF

#### Naive Bayes - Titanic Survival (`unsuperwised/naive-based-titatic-usecase/`)
- **Survival Prediction**: Titanic passenger survival analysis
- **Historical Dataset**: Real-world classification problem
- **Feature Analysis**: Age, class, gender impact on survival

## ğŸš€ Advanced Topics

### ğŸ› Regularization Techniques (`L1&L2Regulisation/`)
- **L1 Regularization (Lasso)**: Feature selection, sparsity
- **L2 Regularization (Ridge)**: Preventing overfitting
- **Elastic Net**: Combining L1 and L2 regularization
- **Dataset**: Melbourne housing data
- **Applications**: High-dimensional data, feature selection

### âš™ï¸ Hyperparameter Tuning (`hyper-params-tunning/`)
- **Grid Search**: Systematic parameter optimization
- **Random Search**: Efficient parameter exploration
- **Cross-Validation**: Robust parameter selection
- **Model Selection**: Finding the best model configuration

### ğŸ­ Ensemble Learning (`insamble-learning/`)
- **Bagging**: Bootstrap aggregating
- **Boosting**: Sequential model improvement
- **Voting Classifiers**: Combining multiple algorithms
- **Stacking**: Meta-learning approaches
- **Dataset**: Diabetes prediction

### ğŸ¯ Dimensionality Reduction (`pca/`)
- **Principal Component Analysis**: Feature space reduction
- **Variance Preservation**: Maintaining data information
- **Visualization**: High-dimensional data plotting
- **Applications**: Data compression, noise reduction

### ğŸ” Outlier Detection (`outlier-removal-IQR/`)
- **IQR Method**: Interquartile range outlier detection
- **Statistical Approach**: Robust outlier identification
- **Data Cleaning**: Improving model performance
- **Dataset**: Heights data analysis

## ğŸ›  Technologies Used

### Core Libraries
- **Python 3.x** - Programming language
- **Jupyter Notebook** - Interactive development environment
- **pandas** - Data manipulation and analysis
- **NumPy** - Numerical computing
- **matplotlib** - Data visualization and plotting

### Machine Learning
- **scikit-learn** - Comprehensive ML library
  - `LinearRegression`, `LogisticRegression` - Regression and classification
  - `DecisionTreeClassifier`, `RandomForestClassifier` - Tree-based algorithms
  - `SVC`, `SVR` - Support Vector Machines
  - `KNeighborsClassifier` - K-Nearest Neighbors
  - `KMeans` - Clustering algorithms
  - `PCA` - Dimensionality reduction
  - `OneHotEncoder`, `StandardScaler` - Data preprocessing
  - `ColumnTransformer` - Feature preprocessing pipelines
  - `GridSearchCV` - Hyperparameter tuning
  - `cross_val_score` - Model validation

### Model Persistence & Deployment
- **joblib** - Efficient model serialization
- **pickle** - Standard Python object serialization

### Specialized Libraries
- **seaborn** - Statistical data visualization
- **plotly** - Interactive plotting
- **scipy** - Scientific computing

## ğŸ Getting Started

### Prerequisites

Make sure you have Python 3.x installed along with the required packages:

```bash
# Essential packages
pip install pandas numpy scikit-learn matplotlib jupyter

# Additional packages for advanced examples
pip install seaborn plotly scipy joblib
```

### Installation

1. Clone this repository:
```bash
git clone <repository-url>
cd machine-learning
```

2. Launch Jupyter Notebook:
```bash
jupyter notebook
```

## âš¡ Quick Start Examples

### ğŸš€ Best Starting Points

#### 1. Complete Prediction Pipeline (Recommended)
```bash
cd superwised/dummy_variable_and_one_hot_encoding
python prediction_example.py
```
**What you'll see**: Complete ML workflow from data loading to predictions with professional output

#### 2. Basic Linear Regression
```bash
# Navigate to basic examples
cd superwised/linear-reg/linear_reg_home
# Open the Jupyter notebook
jupyter notebook linear-reg_home_area.ipynb
```

#### 3. Classification Example
```bash
cd superwised/logistic_regression
jupyter notebook index.ipynb
```

#### 4. Clustering Example
```bash
cd unsuperwised/k\ mean\ clustering
jupyter notebook
```

### ğŸ“Š Expected Output (Prediction Example)
```
ğŸ  House Price Prediction with Categorical Data
==================================================
Dataset Overview:
Shape: (13, 3)
Columns: ['town', 'area', 'price']

ğŸ”® Making Predictions
Example 1: Single Prediction
   Input: Monroe Township, 2800 sq ft
   Predicted Price: $612,500.00

Example 2: Multiple Predictions
   1. Monroe Township, 2500 sq ft â†’ $587,500.00
   2. West Windsor, 3500 sq ft â†’ $675,000.00
   3. Robinsville, 2800 sq ft â†’ $598,750.00
```

### ğŸ¯ Key Learning Concepts

#### âœ… **Consistent Preprocessing**
- Use the same preprocessor for training and prediction
- Transform new data with `preprocessor.transform()` (not `fit_transform()`)

#### âœ… **Proper Data Format**
- Create DataFrames with identical column structure
- Maintain feature order and naming consistency

#### âœ… **Multiple Scenarios**
- Single predictions for individual cases
- Batch predictions for multiple inputs
- Comparison analysis across different categories

#### âœ… **Production-Ready Pipelines**
- Use `ColumnTransformer` for mixed data types
- Implement proper error handling and validation
- Create reusable prediction functions

## ğŸ“Š Featured Datasets

### ğŸ  Real Estate Data
- **Home Prices**: Area, bedrooms, age, town, price
- **Applications**: Regression, categorical encoding, feature engineering
- **Complexity Levels**: Single variable â†’ Multiple variables â†’ Categorical features

### ğŸ“ˆ Financial & Economic Data
- **Canada Income**: Historical per capita income (1970-2016)
- **Insurance**: Customer demographics and purchase decisions
- **Applications**: Time series analysis, classification

### ğŸš— Automotive Data
- **Car Prices**: Model, mileage, age, price
- **Applications**: Categorical encoding, price prediction

### ğŸ¥ Healthcare Data
- **Diabetes**: Patient health metrics and diagnosis
- **Applications**: Classification, ensemble learning

### ğŸ‘¥ HR & Business Data
- **Employee Retention**: Job satisfaction, performance, turnover
- **Titanic Survival**: Passenger demographics and survival
- **Applications**: Business analytics, historical analysis

### ğŸ”¢ Classic ML Datasets
- **Iris Flowers**: Sepal/petal measurements, species classification
- **Digit Recognition**: Handwritten digit images
- **Heights**: Statistical distribution analysis
- **Applications**: Benchmarking, algorithm comparison

## ğŸ“ Learning Outcomes

### ğŸ“š Fundamental Concepts
- **Supervised vs Unsupervised Learning**: Understanding different ML paradigms
- **Regression vs Classification**: Predicting continuous vs categorical outcomes
- **Model Evaluation**: Cross-validation, train/test splits, performance metrics
- **Overfitting & Underfitting**: Bias-variance tradeoff, regularization

### ğŸ”§ Technical Skills
- **Data Preprocessing**: Cleaning, encoding, scaling, feature engineering
- **Algorithm Implementation**: Linear/logistic regression, trees, SVM, clustering
- **Pipeline Development**: End-to-end ML workflows, preprocessing pipelines
- **Model Deployment**: Saving, loading, and using trained models

### ğŸ“Š Data Handling
- **Mixed Data Types**: Numerical, categorical, text data preprocessing
- **Missing Values**: Imputation strategies, data quality assessment
- **Feature Engineering**: Creating, selecting, and transforming features
- **Dimensionality Reduction**: PCA, feature selection techniques

### ğŸš€ Advanced Topics
- **Ensemble Methods**: Combining multiple models for better performance
- **Hyperparameter Tuning**: Grid search, random search, optimization
- **Regularization**: L1/L2 penalties, preventing overfitting
- **Outlier Detection**: Identifying and handling anomalous data

### ğŸ’¼ Practical Applications
- **Real-World Projects**: Business problems, data-driven decision making
- **Production Pipelines**: Scalable, maintainable ML systems
- **Best Practices**: Code organization, documentation, reproducibility

## ğŸ¤ Contributing

We welcome contributions! Here are ways you can help:

### ğŸ†• New Examples
- Add examples for new algorithms (Neural Networks, XGBoost, etc.)
- Create industry-specific use cases (Finance, Healthcare, Marketing)
- Implement advanced techniques (Deep Learning, NLP, Computer Vision)

### ğŸ“ˆ Improvements
- Enhance existing notebooks with better explanations
- Add more comprehensive error handling
- Improve data visualization and insights
- Optimize code performance and readability

### ğŸ“š Documentation
- Expand prediction guides and tutorials
- Add troubleshooting sections
- Create video tutorials or blog posts
- Translate documentation to other languages

### ğŸ”§ Technical Enhancements
- Add automated testing for notebooks
- Create Docker containers for easy setup
- Implement CI/CD pipelines
- Add interactive web demos

### ğŸ“Š Datasets
- Contribute new, interesting datasets
- Clean and document existing datasets
- Add data validation and quality checks

## ğŸŒŸ Acknowledgments

- **scikit-learn community** for excellent documentation and examples
- **Kaggle** for providing diverse datasets
- **Jupyter Project** for the interactive notebook environment
- **Open source contributors** who make machine learning accessible

## ğŸ“ License

This project is open source and available under the [MIT License](LICENSE).

---

**Happy Learning! ğŸš€** Start with the prediction example and explore the fascinating world of machine learning!